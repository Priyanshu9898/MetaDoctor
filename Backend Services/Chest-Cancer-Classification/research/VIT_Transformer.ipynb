{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1432479,"sourceType":"datasetVersion","datasetId":839140}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-24T13:52:53.936657Z","iopub.execute_input":"2023-12-24T13:52:53.937052Z","iopub.status.idle":"2023-12-24T13:52:54.297414Z","shell.execute_reply.started":"2023-12-24T13:52:53.937008Z","shell.execute_reply":"2023-12-24T13:52:54.296606Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\nfrom transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\nfrom transformers import TrainingArguments, Trainer\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:52:54.299488Z","iopub.execute_input":"2023-12-24T13:52:54.299938Z","iopub.status.idle":"2023-12-24T13:53:00.312214Z","shell.execute_reply.started":"2023-12-24T13:52:54.299908Z","shell.execute_reply":"2023-12-24T13:53:00.311265Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define data directories\ntrain_dir = '/kaggle/input/chest-ctscan-images/Data/train'\nvalid_dir = '/kaggle/input/chest-ctscan-images/Data/valid'\ntest_dir = '/kaggle/input/chest-ctscan-images/Data/test'","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:00.313555Z","iopub.execute_input":"2023-12-24T13:53:00.314617Z","iopub.status.idle":"2023-12-24T13:53:00.319212Z","shell.execute_reply.started":"2023-12-24T13:53:00.314580Z","shell.execute_reply":"2023-12-24T13:53:00.318170Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Image transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:00.320545Z","iopub.execute_input":"2023-12-24T13:53:00.321259Z","iopub.status.idle":"2023-12-24T13:53:00.332549Z","shell.execute_reply.started":"2023-12-24T13:53:00.321210Z","shell.execute_reply":"2023-12-24T13:53:00.331827Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\nvalid_dataset = datasets.ImageFolder(valid_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=transform)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:00.334910Z","iopub.execute_input":"2023-12-24T13:53:00.335176Z","iopub.status.idle":"2023-12-24T13:53:00.360756Z","shell.execute_reply.started":"2023-12-24T13:53:00.335153Z","shell.execute_reply":"2023-12-24T13:53:00.360069Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=32)\ntest_loader = DataLoader(test_dataset, batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:00.361586Z","iopub.execute_input":"2023-12-24T13:53:00.361809Z","iopub.status.idle":"2023-12-24T13:53:00.367283Z","shell.execute_reply.started":"2023-12-24T13:53:00.361789Z","shell.execute_reply":"2023-12-24T13:53:00.365967Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Load pre-trained ViT model\nmodel = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=4)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:00.368466Z","iopub.execute_input":"2023-12-24T13:53:00.368784Z","iopub.status.idle":"2023-12-24T13:53:00.978514Z","shell.execute_reply.started":"2023-12-24T13:53:00.368754Z","shell.execute_reply":"2023-12-24T13:53:00.977584Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training settings\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:00.979778Z","iopub.execute_input":"2023-12-24T13:53:00.980079Z","iopub.status.idle":"2023-12-24T13:53:03.494291Z","shell.execute_reply.started":"2023-12-24T13:53:00.980051Z","shell.execute_reply":"2023-12-24T13:53:03.493370Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"ViTForImageClassification(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViTLayer(\n          (attention): ViTAttention(\n            (attention): ViTSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n  )\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=1e-4)\n\n# Define loss function\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:03.495551Z","iopub.execute_input":"2023-12-24T13:53:03.495905Z","iopub.status.idle":"2023-12-24T13:53:03.505117Z","shell.execute_reply.started":"2023-12-24T13:53:03.495873Z","shell.execute_reply":"2023-12-24T13:53:03.504287Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Training loop\nnum_epochs = 50\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for images, labels in train_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        loss = loss_fn(outputs.logits, labels)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n    # Validation loop\n    model.eval()\n    total_correct = 0\n    total_samples = 0\n    with torch.no_grad():\n        for images, labels in valid_loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs.logits, 1)\n            total_correct += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n    accuracy = total_correct / total_samples\n    print(f\"Validation Accuracy: {accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-24T13:53:03.506117Z","iopub.execute_input":"2023-12-24T13:53:03.506385Z","iopub.status.idle":"2023-12-24T14:13:56.195650Z","shell.execute_reply.started":"2023-12-24T13:53:03.506356Z","shell.execute_reply":"2023-12-24T14:13:56.194660Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch [1/50], Loss: 1.0965\nValidation Accuracy: 0.4722\nEpoch [2/50], Loss: 0.6617\nValidation Accuracy: 0.7361\nEpoch [3/50], Loss: 0.3389\nValidation Accuracy: 0.6389\nEpoch [4/50], Loss: 0.2995\nValidation Accuracy: 0.8611\nEpoch [5/50], Loss: 0.1181\nValidation Accuracy: 0.7361\nEpoch [6/50], Loss: 0.0746\nValidation Accuracy: 0.8194\nEpoch [7/50], Loss: 0.0592\nValidation Accuracy: 0.8750\nEpoch [8/50], Loss: 0.0647\nValidation Accuracy: 0.8611\nEpoch [9/50], Loss: 0.0498\nValidation Accuracy: 0.6667\nEpoch [10/50], Loss: 0.0813\nValidation Accuracy: 0.7222\nEpoch [11/50], Loss: 0.0597\nValidation Accuracy: 0.7778\nEpoch [12/50], Loss: 0.0295\nValidation Accuracy: 0.8611\nEpoch [13/50], Loss: 0.0262\nValidation Accuracy: 0.7917\nEpoch [14/50], Loss: 0.0224\nValidation Accuracy: 0.8750\nEpoch [15/50], Loss: 0.0201\nValidation Accuracy: 0.8750\nEpoch [16/50], Loss: 0.0185\nValidation Accuracy: 0.8750\nEpoch [17/50], Loss: 0.0170\nValidation Accuracy: 0.8750\nEpoch [18/50], Loss: 0.0159\nValidation Accuracy: 0.8750\nEpoch [19/50], Loss: 0.0156\nValidation Accuracy: 0.8750\nEpoch [20/50], Loss: 0.0144\nValidation Accuracy: 0.8750\nEpoch [21/50], Loss: 0.0139\nValidation Accuracy: 0.8750\nEpoch [22/50], Loss: 0.0129\nValidation Accuracy: 0.8750\nEpoch [23/50], Loss: 0.0123\nValidation Accuracy: 0.8750\nEpoch [24/50], Loss: 0.0116\nValidation Accuracy: 0.8750\nEpoch [25/50], Loss: 0.0112\nValidation Accuracy: 0.8750\nEpoch [26/50], Loss: 0.0106\nValidation Accuracy: 0.8750\nEpoch [27/50], Loss: 0.0102\nValidation Accuracy: 0.8750\nEpoch [28/50], Loss: 0.0097\nValidation Accuracy: 0.8750\nEpoch [29/50], Loss: 0.0097\nValidation Accuracy: 0.8750\nEpoch [30/50], Loss: 0.0090\nValidation Accuracy: 0.8750\nEpoch [31/50], Loss: 0.0087\nValidation Accuracy: 0.8750\nEpoch [32/50], Loss: 0.0084\nValidation Accuracy: 0.8750\nEpoch [33/50], Loss: 0.0083\nValidation Accuracy: 0.8750\nEpoch [34/50], Loss: 0.0076\nValidation Accuracy: 0.8750\nEpoch [35/50], Loss: 0.0078\nValidation Accuracy: 0.8750\nEpoch [36/50], Loss: 0.0075\nValidation Accuracy: 0.8750\nEpoch [37/50], Loss: 0.0072\nValidation Accuracy: 0.8750\nEpoch [38/50], Loss: 0.0068\nValidation Accuracy: 0.8750\nEpoch [39/50], Loss: 0.0069\nValidation Accuracy: 0.8750\nEpoch [40/50], Loss: 0.0064\nValidation Accuracy: 0.8750\nEpoch [41/50], Loss: 0.0066\nValidation Accuracy: 0.8750\nEpoch [42/50], Loss: 0.0062\nValidation Accuracy: 0.8750\nEpoch [43/50], Loss: 0.0065\nValidation Accuracy: 0.8750\nEpoch [44/50], Loss: 0.0058\nValidation Accuracy: 0.8750\nEpoch [45/50], Loss: 0.0057\nValidation Accuracy: 0.8750\nEpoch [46/50], Loss: 0.0058\nValidation Accuracy: 0.8750\nEpoch [47/50], Loss: 0.0059\nValidation Accuracy: 0.8750\nEpoch [48/50], Loss: 0.0055\nValidation Accuracy: 0.8750\nEpoch [49/50], Loss: 0.0052\nValidation Accuracy: 0.8750\nEpoch [50/50], Loss: 0.0051\nValidation Accuracy: 0.8750\n","output_type":"stream"}]},{"cell_type":"code","source":"# Save the model\nmodel_save_path = 'vit_model.pth'\ntorch.save(model.state_dict(), model_save_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:13:56.196841Z","iopub.execute_input":"2023-12-24T14:13:56.197150Z","iopub.status.idle":"2023-12-24T14:13:56.877169Z","shell.execute_reply.started":"2023-12-24T14:13:56.197123Z","shell.execute_reply":"2023-12-24T14:13:56.876156Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Test loop and metrics calculation\nmodel.eval()\nall_labels = []\nall_predictions = []\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.logits, 1)\n        all_labels.extend(labels.cpu().numpy())\n        all_predictions.extend(predicted.cpu().numpy())\n\n# Calculate metrics\naccuracy = accuracy_score(all_labels, all_predictions)\nprecision = precision_score(all_labels, all_predictions, average='macro')\nrecall = recall_score(all_labels, all_predictions, average='macro')\nf1 = f1_score(all_labels, all_predictions, average='macro')\n\nprint(f\"Test Metrics:\\nAccuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\nprint(\"Training complete. Model saved to\", model_save_path)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:13:56.878706Z","iopub.execute_input":"2023-12-24T14:13:56.879330Z","iopub.status.idle":"2023-12-24T14:14:02.882187Z","shell.execute_reply.started":"2023-12-24T14:13:56.879297Z","shell.execute_reply":"2023-12-24T14:14:02.881250Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Test Metrics:\nAccuracy: 0.9175, Precision: 0.9288, Recall: 0.9251, F1 Score: 0.9252\nTraining complete. Model saved to vit_model.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}